{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "fiction-chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aNNSDyR1AZfF",
        "yD03mw8212_O",
        "ddX3MEh4KPkr",
        "YDsg_fUm1pW6"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHID462m95eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0384dc9e-39c4-4a0d-b423-940880fee20b"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device  #should be 'cuda'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwbvtIHZHGdg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "b02528aa-559c-4d55-c244-e50b7663a5bf"
      },
      "source": [
        "!git clone https://github.com/RVirmoors/fiction-chatbot.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fiction-chatbot'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 34 (delta 10), reused 29 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czYCe5T6zgoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cab273c8-f043-41af-f6f3-1780298812f2"
      },
      "source": [
        "%cd fiction-chatbot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fiction-chatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uYX7jloprda",
        "colab_type": "text"
      },
      "source": [
        "## DeepPavlov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9gk9TeppuwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "dda19889-e4a1-4b04-b55e-ac778f635f40"
      },
      "source": [
        "!pip install deeppavlov\n",
        "!python -m deeppavlov install en_odqa_infer_wiki"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.2.3 thinc-7.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxhj86SiqLm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load https://github.com/deepmipt/DeepPavlov/blob/0.1.6/deeppavlov/configs/odqa/en_odqa_infer_wiki.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebMotOzWUsL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ERROR path not found\n",
        "\n",
        "from deeppavlov import configs\n",
        "from deeppavlov.core.commands.train import train_evaluate_model_from_config\n",
        "\n",
        "train_evaluate_model_from_config(configs.doc_retrieval.en_ranker_tfidf_wiki, download=True)\n",
        "train_evaluate_model_from_config(configs.squad.multi_squad_noans, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0eI8tz1VIgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "6ca21f13-4ccf-407d-f081-0491134de018"
      },
      "source": [
        "# interact with the model\n",
        "\n",
        "from deeppavlov import configs\n",
        "from deeppavlov.core.commands.infer import build_model\n",
        "\n",
        "odqa = build_model(configs.odqa.en_odqa_infer_wiki, load_trained=True)\n",
        "\n",
        "result = odqa(['What is the name of Darth Vader\\'s son?'])\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "2020-07-23 21:02:44.559 ERROR in 'deeppavlov.core.common.params'['params'] at line 112: Exception in <class 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer.HashingTfIdfVectorizer'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deeppavlov/core/common/params.py\", line 106, in from_params\n",
            "    component = obj(**dict(config_params, **kwargs))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deeppavlov/models/vectorizers/hashing_tfidf_vectorizer.py\", line 80, in __init__\n",
            "    self.tfidf_matrix, opts = self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/deeppavlov/models/vectorizers/hashing_tfidf_vectorizer.py\", line 262, in load\n",
            "    raise FileNotFoundError(\"HashingTfIdfVectorizer path doesn't exist!\")\n",
            "FileNotFoundError: HashingTfIdfVectorizer path doesn't exist!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5d31758b8b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0modqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men_odqa_infer_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_trained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'What is the name of Darth Vader\\'s son?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeppavlov/core/commands/infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, download, serialized)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcomponent_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_serialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeppavlov/core/common/params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, serialized, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0m_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0m_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeppavlov/core/commands/infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, download, serialized)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcomponent_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_serialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeppavlov/core/common/params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, serialized, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0m_refs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeppavlov/models/vectorizers/hashing_tfidf_vectorizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, hash_size, doc_index, save_path, load_path, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngram_range'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hash_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeppavlov/models/vectorizers/hashing_tfidf_vectorizer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HashingTfIdfVectorizer path doesn't exist!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading tfidf matrix from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: HashingTfIdfVectorizer path doesn't exist!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "votxLK_br15D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "5b95ae85-b178-450e-9fd1-4a8acbc24aa2"
      },
      "source": [
        "from deeppavlov import configs\n",
        "from deeppavlov.core.common.file import read_json\n",
        "from deeppavlov import configs, train_model\n",
        "\n",
        "model_config = read_json(configs.doc_retrieval.en_ranker_tfidf_wiki)\n",
        "model_config[\"dataset_reader\"][\"data_path\"] = \"data\"\n",
        "model_config[\"dataset_reader\"][\"dataset_format\"] = \"txt\"\n",
        "ranker = train_model(model_config)\n",
        "\n",
        "docs = ranker(['cerebellum'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-07-23 12:40:19.126 INFO in 'deeppavlov.dataset_readers.odqa_reader'['odqa_reader'] at line 57: Reading files...\n",
            "2020-07-23 12:40:19.128 INFO in 'deeppavlov.dataset_readers.odqa_reader'['odqa_reader'] at line 134: Building the database...\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\n",
            "2it [00:00, 30.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 25.13it/s]\n",
            "2020-07-23 12:40:19.318 INFO in 'deeppavlov.dataset_iterators.sqlite_iterator'['sqlite_iterator'] at line 57: Connecting to database, path: /root/.deeppavlov/downloads/odqa/enwiki.db\n",
            "2020-07-23 12:40:19.319 INFO in 'deeppavlov.dataset_iterators.sqlite_iterator'['sqlite_iterator'] at line 112: SQLite iterator: The size of the database is 2 documents\n",
            "2020-07-23 12:40:19.762 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 153: Tokenizing batch...\n",
            "2020-07-23 12:40:40.455 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 155: Counting hash...\n",
            "2020-07-23 12:40:41.16 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 214: Saving tfidf matrix to /root/.deeppavlov/models/odqa/enwiki_tfidf_matrix.npz\n",
            "2020-07-23 12:40:43.780 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 264: Loading tfidf matrix from /root/.deeppavlov/models/odqa/enwiki_tfidf_matrix.npz\n",
            "2020-07-23 12:40:44.523 INFO in 'deeppavlov.models.vectorizers.hashing_tfidf_vectorizer'['hashing_tfidf_vectorizer'] at line 264: Loading tfidf matrix from /root/.deeppavlov/models/odqa/enwiki_tfidf_matrix.npz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whrymDisse7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('trained_ranker.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(model_config, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "odqa_config = read_json(configs.odqa.en_odqa_infer_wiki)\n",
        "odqa_config[\"chainer\"][\"pipe\"][\"config_path\"] = \n",
        "\n",
        "odqa = build_model(ranker, download = False)\n",
        "a = odqa([\"what is tuberculosis ?\"])\n",
        "# ['a disease for which a new drug is desperately needed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbaXevvOu6jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep3UUlVWs1XD",
        "colab_type": "text"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNNSDyR1AZfF",
        "colab_type": "text"
      },
      "source": [
        "#### Step 1: Prep the text for analysis in Python\n",
        "\n",
        "The very first step is to get a copy of all of the books to analyze - I was able to get some .txt files, so that is reflected in the code below.\n",
        "\n",
        "No matter what format your files are in, however, the next step is the same: go from that file format to a string of characters that we can process with Python and NLTK.   This is done with the `read_file` function, below.\n",
        "\n",
        "To keep what comes next clear, we'll only read in just one book of the series (#1) unless otherwise stated.  In the way I've structured my code, this is indicated with the `num` variable that's passed in. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERPQFdWDAZfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(num):\n",
        "    text = ''\n",
        "    with open('data/text'+str(num)+'.txt', 'rt', encoding=\"utf8\") as file_in:\n",
        "        for line in file_in:\n",
        "            text = text + line\n",
        "    return text\n",
        "\n",
        "# here we create a variable called book_content and make it equal to the value of the read_file function for book 1\n",
        "book_content = read_file(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9iWXGKEAZgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "1e9d1dcc-9eca-4359-a81c-05ce7ff77204"
      },
      "source": [
        "print(\"Let's see what's in the book_content variable.  We'll work with this example through most of this post.\")\n",
        "print(book_content[149756:149980])\n",
        "print()\n",
        "print('*' * 20)\n",
        "print(\"Looks good!  We can also double check the format of this variable is a string...\")\n",
        "print(type(book_content))\n",
        "print(\"And how many characters it has...\")\n",
        "print(len(book_content))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's see what's in the book_content variable.  We'll work with this example through most of this post.\n",
            "rienced, just new terms. Basically this is a religious experience, but also it is more because we are no longer a religious world; I am a secular person in a secular society and must understand my experiences in this context\n",
            "\n",
            "********************\n",
            "Looks good!  We can also double check the format of this variable is a string...\n",
            "<class 'str'>\n",
            "And how many characters it has...\n",
            "2425076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD03mw8212_O",
        "colab_type": "text"
      },
      "source": [
        "### install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFDcAeFN1oTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1yR4fGvIlW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "99414ae4-a430-416a-9b6d-dd16b90781ac"
      },
      "source": [
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizers               0.8.1rc2       \n",
            "transformers             3.0.2          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIb5IMEO192J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_squad.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --train_file squad_dir/train-v1.1.json \\\n",
        "  --predict_file squad_dir/dev-v1.1.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 2.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir /tmp/debug_squad/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddX3MEh4KPkr",
        "colab_type": "text"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVLgNDi8LBrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "82e7de8b-8451-49a5-c050-5129fa5ab634"
      },
      "source": [
        "question=\"\"\n",
        "input(question)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGncNxwBCYdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb88acc7-6b68-441d-b733-88148f068566"
      },
      "source": [
        "!python run_squad.py \\\n",
        "  --model_type distilbert \\\n",
        "  --model_name_or_path distilbert-base-cased-distilled-squad \\\n",
        "  --do_lower_case \\\n",
        "  --train_file squad_dir/train-v1.1.json \\\n",
        "  --predict_file squad_dir/dev-v1.1.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 2.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir /tmp/debug_squad/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/23/2020 10:30:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "07/23/2020 10:30:14 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-distilled-squad-config.json from cache at /root/.cache/torch/transformers/c2341a51039a311cb3c7dc71b3d21970e6a127876f067f379f8bcd77ef870389.6a09face0659d64f93c9919f323e2ad4543ca9af5d2417b1bfb1a36f2f6b94a4\n",
            "07/23/2020 10:30:14 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": true,\n",
            "  \"tie_weights_\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "07/23/2020 10:30:14 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-distilled-squad-config.json from cache at /root/.cache/torch/transformers/c2341a51039a311cb3c7dc71b3d21970e6a127876f067f379f8bcd77ef870389.6a09face0659d64f93c9919f323e2ad4543ca9af5d2417b1bfb1a36f2f6b94a4\n",
            "07/23/2020 10:30:14 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": true,\n",
            "  \"tie_weights_\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "07/23/2020 10:30:14 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /root/.cache/torch/transformers/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "07/23/2020 10:30:15 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/distilbert-base-cased-distilled-squad-pytorch_model.bin from cache at /root/.cache/torch/transformers/3efcb155a9475fe6b9318b8a8d5278bce1972d30291f97f2a8faeb50d02acabc.087b9fac49619019e540876a2d8ecb497884246b5aa8c9e8b7a0292cfbbe7c52\n",
            "07/23/2020 10:30:18 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing DistilBertForQuestionAnswering.\n",
            "\n",
            "07/23/2020 10:30:18 - INFO - transformers.modeling_utils -   All the weights of DistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertForQuestionAnswering for predictions without further training.\n",
            "07/23/2020 10:30:20 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='distilbert-base-cased-distilled-squad', model_type='distilbert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=2.0, output_dir='/tmp/debug_squad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=12, predict_file='squad_dir/dev-v1.1.json', save_steps=500, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='squad_dir/train-v1.1.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "07/23/2020 10:30:20 - INFO - __main__ -   Results: {}\n",
            "YOU:who are you?\n",
            "Traceback (most recent call last):\n",
            "  File \"run_squad.py\", line 916, in <module>\n",
            "    main()\n",
            "  File \"run_squad.py\", line 903, in main\n",
            "    nlp = pipeline('question-answering')\n",
            "NameError: name 'pipeline' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0a1oW5yAM-0",
        "colab_type": "text"
      },
      "source": [
        "prepare data json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUB101jZAO3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b-7VBfP8_X_0",
        "colab": {}
      },
      "source": [
        "!python run_squad.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --data_dir data/ \\\n",
        "  --do_lower_case \\\n",
        "  --train_file fiction.json \\\n",
        "  --predict_file fiction.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 2.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir /tmp/debug_squad/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDsg_fUm1pW6",
        "colab_type": "text"
      },
      "source": [
        "### BOSSY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIIx5WpuAZgl",
        "colab_type": "text"
      },
      "source": [
        "Now that we've read the text in and we have a giant string, we need to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4mvgA_jDpbS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e1a9c159-0c03-4548-842c-a02c3197e897"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7Edy06AZgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "def tokenize_lower_text(book_content):\n",
        "    tokenized = word_tokenize(book_content)\n",
        "    tokenized = [word.lower() for word in tokenized]\n",
        "    return tokenized\n",
        "\n",
        "tokenized = tokenize_lower_text(book_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-fskY5ZAZhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d3717577-267b-4e77-c78c-d8faa174312c"
      },
      "source": [
        "# Let's check our output - we should see a list of lower case words, same text as before.\n",
        "print(tokenized[33746:33800])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sidereal', 'space', '.', 'i', 'believe', 'it', '.', 'this', 'is', 'the', 'catalyst', '.', 'the', 'valuable', 'aspect', 'of', 'the', 'external', 'catalyst', 'is', 'that', 'it', 'keeps', 'the', 'process', 'within', 'the', 'control', 'of', 'who', 'it', 'is', 'who', 'controls', 'these', 'things', ';', 'it', 'is', \"n't\", 'going', 'to', 'simply', 'occur', 'at', 'a', 'random', 'time', 'for', 'no', 'reason', 'at', 'all', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz-UUEYnAZh1",
        "colab_type": "text"
      },
      "source": [
        "That's it for step 1!  Now that we've gotten the text ready for processing, we need to seperate out the parts we need and group them to prep for the analysis.  Let's do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvrb35coAZh7",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2: Isolate and group the parts of the text that need to be analyzed\n",
        "\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PykWx6eHAZiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_text(t):\n",
        "    open_q = '``'\n",
        "    close_q = \"''\"\n",
        "    found_q = False # this will be used to break the while loop below\n",
        "    # current will hold words until an open quote is found\n",
        "    current = []\n",
        "    \n",
        "    parsed_dialog = [] \n",
        "    parsed_narrative = []\n",
        "    length = len(t)\n",
        "    i = 0\n",
        "\n",
        "    while i < length:\n",
        "        word = t[i]\n",
        "        \n",
        "        if word != open_q and word != close_q:\n",
        "            current.append(word)\n",
        "\n",
        "        elif word == open_q or word == close_q:\n",
        "            parsed_narrative.append(current)\n",
        "\n",
        "            current = []\n",
        "            current.append(word)\n",
        "\n",
        "            while found_q == False and i < length-1:\n",
        "                i += 1\n",
        "                if t[i] != close_q:\n",
        "                    current.append(t[i])\n",
        "                else:\n",
        "                    current.append(t[i])\n",
        "                    parsed_dialog.append(current)\n",
        "                    current = []\n",
        "                    found_q = True\n",
        "        \n",
        "        found_q = False\n",
        "        i += 1\n",
        "        \n",
        "    return (parsed_dialog, parsed_narrative)\n",
        "\n",
        "\n",
        "parsed_dialog, parsed_narrative = parse_text(tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfumOaUKAZik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "5d7b6e16-fdec-4fcd-bf1b-a968b271a4d8"
      },
      "source": [
        "# Let's take a look at what our original selection looks like now.\n",
        "print(\"parsed_narrative:\", parsed_narrative[711:715])\n",
        "print()\n",
        "print(\"parsed_dialog:\",parsed_dialog[711:714])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parsed_narrative: [['and', 'the', 'lights', 'are', 'in', 'my', 'tv', 'set', '.', 'a', 'circuit', 'few', 'people', 'know', 'about', '.', 'nor', 'are', 'they', 'interested', '.', 'it', 'is', 'my', 'set', ',', 'my', 'discovery', ',', 'my', 'excitement', '.', 'analysis', ':'], ['power', 'flowing', 'through', 'an', 'electrical', '(', 'wiring', ')', 'circuit', 'for', 'the', 'first', 'time', 'to', 'light', 'up', 'lights', 'is', 'a', 'good', 'mechanical', 'analog', 'for', 'first', 'neural', 'firing', 'along', 'a', 'circuit', 'of', 'the', 'brain', '.', 'the', 'rod', 'and', 'cone-base', 'resemble', 'the', 'rod', 'of', 'a', 'nuclear', 'reactor', ';', 'atomic', 'power', ':', 'a', 'good', 'metaphor', 'for', 'the', 'source', 'of', 'psychic', 'energy', '.', 'i', \"'m\", 'going', 'to', 'know', 'while', 'he', \"'s\", 'still', 'invisible', ';', 'the', 'others', 'wo', \"n't\", 'know', 'until', 'later', ',', 'until', 'he', 'becomes', 'visible', '.', 'interestingly', ',', 'the', 'dream', 'placed', 'me', 'back', 'with', 'my', 'high', 'school', 'friends', ',', 'which', 'sets', 'it', 'circa', 'the', 'time', 'the', 'voice', 'explained', 'the', 'physics', 'test', 'to', 'me', ';', 'perhaps', 'that', 'was', 'when', 'i', 'discovered', 'i', 'had', 'that', 'unusual', 'circuit', '(', 'in', 'my', 'head', ')', '.', '[', '...', ']', 'it', 'has', 'been', 'some', 'time', 'since', 'i', 'developed', 'any', 'conflicting', 'theories', 'about', 'my', 'experience', ';', 'now', 'it', \"'s\", 'an', 'elaboration', 'and', 'a', 'filling', 'in', ',', 'lapidary-wise', ',', 'of', 'detail', '.', 'i', 'have', 'created', 'a', 'consistent', 'explanation', 'based', 'on', 'the', 'experience', 'and', 'on', 'research', '.', 'i', 'doubt', 'if', 'it', 'ever', 'will', 'undergo', 'any', 'substantial', 'modifications', '.', 'it', 'was', 'an', 'epiphany', ';', 'that', 'much', 'is', 'certain', ':', 'an', 'epiphany', 'rather', 'than', 'a', 'theophany', '.', 'throughout', ',', 'the', 'key', 'concepts', 'are', 'greek', ',', 'the', 'key', 'terms', 'are', 'greek', ';', 'it', 'is', 'greek', 'christianity', 'evolving', 'out', 'of', 'plato', '.', 'any', 'other', 'language—other', 'than', 'greek—would', 'be', 'out', 'of', 'place', 'and', 'make', 'no', 'sense', ';', 'the', 'greek', 'words', 'i', 'heard', 'are', 'the', 'cornerstone', ',', 'the', 'key', 'to', 'the', 'cypher', ',', 'and', 'even', 'perhaps', 'a', 'gracious', 'act', 'toward', 'me', 'to', 'assist', 'me', 'in', 'unlocking', 'the', 'entire', 'picture', '.', 'there', 'is', 'only', 'one', 'important', 'issue', 'that', 'i', \"'m\", 'not', 'sure', 'of', ':', 'has', 'holy', 'wisdom', 'who', 'visited', 'me', 'been', 'present', 'during', 'the', 'past', '2,000', 'years', ',', 'or', 'was', 'there', 'an', 'ellipsis', ',', 'and', 'now', 'she/he/it', 'has', 'returned', 'to', 'man', 'to', 'assist', 'him', '?', 'the', 'memory', 'of', 'the', 'spirit', 'contained', 'nothing', 'between', 'the', 'first', 'century', 'a.d.', 'and', 'world', 'war', 'one', ';', 'that', 'is', 'a', 'clue', 'that', 'an', 'ellipsis', 'did', 'indeed', 'occur', '.', 'also', ',', 'there', 'are', 'no', 'reports', 'that', 'i', 'can', 'find', ',', 'down', 'through', 'the', 'ages', ',', 'of', 'a', 'neoplatonistic', 'total', 'print-out', 'such', 'as', 'i', 'got', ',', 'the', 'grand', 'sum', 'of', 'neoplatonistic', 'mystery', 'gnosis', '.', 'surely', 'someone', 'would', 'have', 'reported', 'it', 'before', 'now', '.', 'i', 'have', 'received', 'the', 'greatest', 'gift', 'which', 'the', 'universe', 'can', 'bestow', '.', 'today', 'i', 'was', 'thinking', 'that', 'as', 'a', 'child', 'i', 'always', 'wanted', 'desperately—i', 'yearned—to', 'hear', 'the'], ['which', 'elijah', 'heard', ',', 'and', 'now', 'i', 'have', 'heard', 'it', '.', 'also', 'i', 'realized', 'that', 'if', 'at', 'the', 'end', 'of', 'my', 'search', 'for', 'god', 'i', 'learned', 'that', 'there', 'is', 'no', 'god', ',', 'then', 'whatever', 'i', 'accomplished', ',', 'experienced', 'or', 'acquired', 'would', 'mean', 'nothing', ';', 'conversely', ',', 'this', 'makes', 'up', 'for', 'anything', 'and', 'everything', ',', 'and', 'creates', 'meaning', 'of', 'an', 'ultimate', 'order', 'in', 'my', 'life', '.', 'the', '3-74', 'experience', 'was'], [';', 'the', 'exegesis', 'which', 'uncovered', 'the', 'significance', 'of', 'the', 'experience', 'is', 'vaster', 'yet—infinite', 'in', 'sum', '.', '*']]\n",
            "\n",
            "parsed_dialog: [['``', 'i', 'have', 'a', 'way', 'of', 'telling', 'when', 'the', 'parousia', 'comes', '!', 'one', 'of', 'my', 'circuits', 'which', 'is', 'usually', 'dormant', 'will', 'light', 'up', '!', 'no', 'one', 'else', 'has', 'it', '!', \"''\"], ['``', 'still', 'small', 'voice', \"''\"], ['``', 'vaster', 'than', 'empires', \"''\"]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bnlzj61AZi4",
        "colab_type": "text"
      },
      "source": [
        "Now we have our narration isolated in the `parsed_narrative` list.  We won't need the `parsed_dialog` list for this project.  \n",
        "\n",
        "Now that we have this list, we need to split the longer passages into single sentences.  This will make analyzing the text a little easier. Let's look at an example of how long these passages of narration can get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlVHdyk8AZi-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fe3aa750-a76b-49ff-b62f-e501664290eb"
      },
      "source": [
        "parsed_dialog, parsed_narrative = parse_text(tokenized)\n",
        "\n",
        "print(\"Sample of narrative list:\", parsed_narrative[6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample of narrative list: ['these', 'metaphysical', ',', 'ethical', ',', 'and', 'ontological', 'themes', 'enmesh', 'his', 'work', ',', 'even', 'from', 'its', 'very', 'beginnings', 'in', 'domestic', 'melodrama', ',', 'science', 'fiction', 'adventure', ',', 'and', 'humor', ',', 'in', 'an', 'atmosphere', 'of', 'philosophical', 'inquiry', '.', 'dick', 'increasingly', 'came', 'to', 'view', 'his', 'earlier', 'writings—specifically', 'his', 'science', 'fiction', 'novels', 'of', 'the', '1960s—as', 'an', 'intricate', 'and', 'unconscious', 'precursor', 'to', 'his', 'visionary', 'insights', '.', 'thus', ',', 'he', 'began', 'to', 'use', 'them', ',', 'as', 'much', 'as', 'any', 'ancient', 'text', 'or', 'the', 'encyclopedia', 'britannica', ',', 'as', 'a', 'source', 'for', 'his', 'investigations', '.', 'never', ',', 'to', 'our', 'knowledge', ',', 'has', 'a', 'novelist', 'borne', 'down', 'with', 'such', 'eccentric', 'concentration', 'on', 'his', 'own', 'oeuvre', ',', 'seeking', 'to', 'crack', 'its', 'code', 'as', 'if', 'his', 'life', 'depended', 'on', 'it', '.', 'the', 'writing', 'in', 'these', 'pages', 'represents', ',', 'perhaps', 'above', 'all', ',', 'a', 'laboratory', 'of', 'interpretation', 'in', 'the', 'most', 'absolute', 'and', 'open-ended', 'sense', 'of', 'the', 'word', '.', 'when', 'dick', 'began', 'to', 'write', 'and', 'publish', 'novels', 'based', 'on', 'the', 'visionary', 'material', 'unearthed', 'in', 'the', 'exegesis', ',', 'he', 'commenced', 'interpreting', 'those', 'as', 'well', '.', 'so', ',', 'as', 'these', 'writings', 'accumulated', ',', 'they', 'also', 'became', 'self-referential', ':', 'the', 'exegesis', 'is', 'a', 'study', 'of', ',', 'among', 'other', 'things', ',', 'itself', '.', 'fully', 'situating', 'this', 'text', \"'s\", 'genesis', 'within', 'the', 'flamboyant', 'and', 'heartbreaking', 'life', 'story', 'of', 'philip', 'k.', 'dick', 'is', 'beyond', 'our', 'reach', 'in', 'this', 'introduction', '.', 'we', 'commend', 'you', 'to', 'lawrence', 'sutin', \"'s\", 'divine', 'invasions', ':', 'a', 'life', 'of', 'philip', 'k.', 'dick', ',', 'published', 'in', '1989', 'and', 'thankfully', 'still', 'in', 'print', '.', 'sutin', \"'s\", 'biography', 'finds', 'its', 'limitations', 'only', 'in', 'the', 'sense', 'that', 'neither', 'he', 'nor', 'any', 'other', 'commentator', 'in', 'the', 'years', 'immediately', 'following', 'dick', \"'s\", 'death', ',', 'however', 'persuaded', 'of', 'the', 'unique', 'relevance', 'and', 'appeal', 'of', 'his', 'writing', ',', 'could', 'have', 'predicted', 'the', 'expansion', 'in', 'its', 'reputation', 'and', 'influence', 'in', 'the', 'subsequent', 'decades', '.', 'what', 'will', 'be', 'needed', 'by', 'a', 'reader', 'coming', 'to', 'the', 'exegesis', ',', 'however', ',', 'whether', 'familiar', 'or', 'not', 'with', 'dick', \"'s\", 'great', 'novels', ',', 'is', 'a', 'brief', 'encapsulation', 'of', 'what', 'both', 'dick', 'and', 'sutin', 'call']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bu_Q0XwAZjW",
        "colab_type": "text"
      },
      "source": [
        "`split_sent` is a helper function (below) that will take the `parsed_narrative` list and split it into sentences. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cZI4u5UAZjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import groupby as gb\n",
        "\n",
        "def split_sent(t):\n",
        "    sent_list = []\n",
        "    for sent in t:\n",
        "        k = [list(sent) for i, sent in gb(sent, lambda item: item=='.')]\n",
        "        for i in k:\n",
        "            if len(i) > 1:\n",
        "                sent_list.append(i)\n",
        "    return (sent_list)\n",
        "\n",
        "narrative_split_sent = split_sent(tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSCQVe6NAZj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fe24a90b-05da-4404-896f-a71fa00896a2"
      },
      "source": [
        "# Let's print the first few entries from the  key to see what it looks like.\n",
        "print(narrative_split_sent[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"'\", \"'\"], ['t', 'o', 'm', 'o', 'r', 'r', 'o', 'w'], ['m', 'o', 'r', 'n', 'i', 'n', 'g'], [\"'\", \"'\"], ['h', 'e'], ['d', 'e', 'c', 'i', 'd', 'e', 'd'], ['`', '`'], [\"'\", 'l', 'l'], ['b', 'e', 'g', 'i', 'n'], ['c', 'l', 'e', 'a', 'r', 'i', 'n', 'g'], ['a', 'w', 'a', 'y'], ['t', 'h', 'e'], ['s', 'a', 'n', 'd'], ['o', 'f'], ['f', 'i', 'f', 't', 'y'], ['t', 'h', 'o', 'u', 's', 'a', 'n', 'd'], ['c', 'e', 'n', 't', 'u', 'r', 'i', 'e', 's'], ['f', 'o', 'r'], ['m', 'y'], ['f', 'i', 'r', 's', 't']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}