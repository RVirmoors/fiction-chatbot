{"cells":[{"cell_type":"markdown","metadata":{"id":"rvzkreKSwW4r"},"source":["# Fiction-based chatbot 2022"]},{"cell_type":"markdown","metadata":{"id":"WVECH3yVv4WL"},"source":["check the GPU you're using"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1660980329673,"user":{"displayName":"Grigore Burloiu","userId":"03230274794780392775"},"user_tz":-180},"id":"U02W5QFcTz8O","outputId":"23cdd6d4-b444-4153-ec09-c6b260257388"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Aug 20 07:25:28 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"uH0AdgRjwD8_"},"source":["mount your Google Drive using the \"Mount Drive\" button in the Files panel\n","\n","replace the path below with your own version: right-click in the browser and \"Copy path\" then paste after `%cd `"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14984,"status":"ok","timestamp":1661017416473,"user":{"displayName":"Grigore Burloiu","userId":"03230274794780392775"},"user_tz":-180},"id":"Oiw8d8UKa4d_","outputId":"bfa96776-bdf4-4638-a85b-74fd439cd92d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 16.6 MB/s \n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 59.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 58.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 13.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 71.6 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 70.6 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.8 MB/s \n","\u001b[?25hRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 huggingface-hub-0.8.1 multiprocess-0.70.13 responses-0.18.0 tokenizers-0.12.1 transformers-4.21.1 urllib3-1.25.11 xxhash-3.0.0\n","/content/drive/MyDrive/projects/2022 - chatbot\n"]}],"source":["!pip install transformers datasets\n","%cd /content/drive/MyDrive/projects/2022 - chatbot"]},{"cell_type":"markdown","metadata":{"id":"cmCJWLyuw7dc"},"source":["## Training\n","\n","Skip this part if you just want to generate using an existing pretrained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6vzC_hVx5lw"},"outputs":[],"source":["# run this to see a list of the possible training params\n","!python run_clm.py --help"]},{"cell_type":"markdown","metadata":{"id":"xQMn0dXrITQ6"},"source":["run the training. Try different values for `num_train_epochs`, `learning_rate`, `gradient_accumulation_steps`..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKblQKDOye7I"},"outputs":[],"source":["!python run_clm.py \\\n","--model_name_or_path \"/content/drive/MyDrive/projects/2022 - chatbot/gpt-combinedplus/checkpoint-4837\" \\\n","--train_file combinedPlus.txt \\\n","--do_train \\\n","--fp16 \\\n","--overwrite_cache \\\n","--output_dir gpt-combinedplus2 \\\n","--learning_rate 2e-06 \\\n","--num_train_epochs 20 \\\n","--gradient_accumulation_steps 1 \\\n","--per_device_train_batch_size 2 \\\n","--save_strategy epoch \\\n","--save_steps 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ITQR43_crZn"},"outputs":[],"source":["# EXTRA FINETUNE - better not?\n","!python run_clm.py \\\n","--model_name_or_path finetuned \\\n","--train_file QA.txt \\\n","--do_train \\\n","--fp16 \\\n","--overwrite_cache \\\n","--ignore_data_skip \\\n","--output_dir finetuned-xtra \\\n","--learning_rate 5e-05 \\\n","--num_train_epochs 60 \\\n","--gradient_accumulation_steps 10 \\\n","--per_device_train_batch_size 2"]},{"cell_type":"markdown","metadata":{"id":"NENIzMCGzWDD"},"source":["## Generate\n","\n","set your main params here. Start with one of the standard models and do some generation. Make sure your finetuned model isn't making things worse!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9J9EBy_znVS"},"outputs":[],"source":["# select the type of architecture: GPT-2 or NEO\n","model_type = 'neo' # 'gpt2' or 'neo'\n","# choose the exact model name\n","model_name = 'neo-combinedplus' #/checkpoint-691'  # 'gpt2' or 'EleutherAI/gpt-neo-125M' or one of your finetuned folders: 'finetuned' etc\n","# add the length of the prompt tokens to match with the mesh-tf generation\n","max_length = 100\n","# number of generated texts\n","num_generate = 3"]},{"cell_type":"markdown","metadata":{"id":"IZ2gT0Ws05LB"},"source":["now specify your (hidden) prompt, which effectively turns your language model into a chatbot. Prompt engineering is an essential part of designing such a system!\n","\n","see also:\n","* https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/\n","* https://generative.ink/posts/methods-of-prompt-programming/\n","* https://chatbotslife.com/openai-gpt-3-tricks-and-tips-72cf48e233f3\n","\n","a single Q+A pair is enough to format GPT/NEO as a chatbot. Try experimenting with multiple Q+A pairs. Customise `head` below as you wish; just make sure it ends in `Q: `\n","\n","*note: the end user WILL NOT SEE this text, it's just part of the model's internal history*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1660306750703,"user":{"displayName":"Grigore Burloiu","userId":"03230274794780392775"},"user_tz":-180},"id":"Ztg89Jp_CZY_","outputId":"d799ec1b-e508-4d61-ec67-7945a3e3b79c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[A chatbot from the future.]\\n\\nQ: Where are you from?\\nA: I come from a place far away, one thousand years in the future.\\nQ: Do aliens exist?\\nA: Yes, but they live very far away.\\nQ: '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["head = \"\"\"[A chatbot from the future.]\n","\n","Q: Where are you from?\n","A: I come from a place far away, one thousand years in the future.\n","Q: Do aliens exist?\n","A: Yes, but they live very far away.\n","Q: \"\"\"\n","\n","head"]},{"cell_type":"markdown","metadata":{"id":"w7C5tY4CEcvl"},"source":["do you want the chatbot to remember Q+A exchanges while it's running?\n","\n","*note: if `num_generate` is not 1, it will remember the last generated answer.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APHJlVU1EnTi"},"outputs":[],"source":["remember_QA = True\n","show_log = True # this is what the user will see, scrolling on the screen"]},{"cell_type":"markdown","metadata":{"id":"43rKqpVd0oRM"},"source":["run the next cell to start chatting!\n","\n","*note: The model may attempt to continue both sides of the conversation, so our `PROCESSED ANSWER` only retains the first generate `A:` line.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":905},"id":"wXRYmF2wzW1o","outputId":"597baef1-3b04-4018-bbcd-c45af94a422f","executionInfo":{"status":"error","timestamp":1660306805640,"user_tz":-180,"elapsed":49976,"user":{"displayName":"Grigore Burloiu","userId":"03230274794780392775"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["model loaded: neo / neo-combinedplus\n","Q: what is love?\n","=== 1 ===\n"," A: Love is a feeling that a person has developed, a longing for a dream that is to come. It is a longing for the universe and that is why they have found you. And I found you. So I am here. I am happy. So I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here\n","PROCESSED ANSWER:\n"," A: Love is a feeling that a person has developed, a longing for a dream that is to come. It is a longing for the universe and that is why they have found you. And I found you. So I am here. I am happy. So I am here. I am here\n","=== 2 ===\n"," A: And if you want to know, I can give you what I think you can.\n","Q: How do you think I should be?\n","A: I think it's really very beautiful. I think it's a magic trick. I think it's very, very easy for me to fool people, and people are very good at it. But I'm afraid of it. You know, I can't stand for it. I'm afraid of it. I'm afraid of it.\n","PROCESSED ANSWER:\n"," A: And if you want to know, I can give you what I think you can.\n","=== 3 ===\n"," A: Love is something that happens in our planet, and that happens to us. It is a matter of our own choice, and it is a matter of our own evolution. It is like a kind of evolutionary process. It is what we call evolution. It is a process in which our evolution is changing our world. I don't wish to speculate on it, but I do think that at the time of the explosion, we had a great number of planets, and they were larger than life\n","PROCESSED ANSWER:\n"," A: Love is something that happens in our planet, and that happens to us. It is a matter of our own choice, and it is a matter of our own evolution. It is like a kind of evolutionary process. It is what we call evolution. It is a process in which our evolution is changing our world. I don't wish to speculate on it, but I do think that the time of the explosion, we had a great number of planets, and they were larger than life\n","LOG (USER SEES THIS):\n"," Q: what is love?\n","A: Love is something that happens in our planet, and that happens to us. It is a matter of our own choice, and it is a matter of our own evolution. It is like a kind of evolutionary process. It is what we call evolution. It is a process in which our evolution is changing our world. I don't wish to speculate on it, but I do think that the time of the explosion, we had a great number of planets, and they were larger than life\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \"\"\"\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-89d99c2e2457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Q: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# https://github.com/Xirider/finetune-gpt2xl\n","# credit to Niels Rogge - https://github.com/huggingface/transformers/issues/10704\n","\n","import torch\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","if model_type == 'gpt2':\n","  from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","  tokenizer.padding_side = \"left\"\n","  tokenizer.pad_token = tokenizer.eos_token\n","  model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).to(device)\n","\n","if model_type == 'neo':\n","  from transformers import GPTNeoForCausalLM, AutoTokenizer\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  model = GPTNeoForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).half().to(\"cuda\")\n","\n","if model:\n","  print(\"model loaded:\", model_type, \"/\", model_name)\n","\n","log = \"\"\n","while True:\n","  prompt = \"\"\n","  while len(prompt) == 0:\n","    prompt = input(\"Q: \")\n","    prompt = str(prompt)\n","  log = log + \"Q: \" + prompt + \"\\n\"\n","  \n","  if remember_QA:\n","    text = head + log\n","  else:\n","    text = head + \"Q: \" + prompt + \"\\n\"\n","\n","  if model_type == 'gpt2':\n","    encoding = tokenizer(text, padding=True, return_tensors='pt').to(device)\n","    max_length = max_length + len(encoding)\n","    with torch.no_grad():\n","        generated_ids = model.generate(\n","            **encoding,\n","            num_return_sequences=num_generate,\n","            do_sample=True,\n","            max_length=max_length,\n","            top_k=50, \n","            top_p=0.95,\n","            use_cache=True\n","          )\n","    generated_texts = tokenizer.batch_decode(\n","        generated_ids, skip_special_tokens=True)\n","\n","  if model_type == 'neo':\n","      ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n","      max_length = max_length + ids.shape[1]\n","      gen_tokens = model.generate(\n","          ids,\n","          num_return_sequences=num_generate,\n","          do_sample=True,\n","          max_length=max_length,\n","          temperature=0.7,\n","          #top_k=50, \n","          #top_p=0.95,\n","          use_cache=True\n","      )\n","      generated_texts = tokenizer.batch_decode(\n","          gen_tokens, skip_special_tokens=True)\n","  #print(generated_texts)\n","  \n","  counter = 1\n","  for generated_text in generated_texts:\n","    generated_text = generated_text.split(text)[1]\n","  \n","    import re\n","    answer = generated_text\n","    # split on all punctuation, take just first after `A:`\n","    # https://bobbyhadz.com/blog/python-split-string-on-punctuation\n","    #answer = re.split( r'()[.!?]', answer)[0]\n","    #https://stackoverflow.com/questions/40736948/regex-string-repetition-of-min-length\n","    answer = re.sub(r\"(.{2,}?)\\1+\", r\"\\1\", answer)\n","    answer = answer.split(\"Q:\")[0]\n","    answer = answer.split(\"\\n\")[0]\n","    #answer = answer + '.'\n","\n","    print(\"===\",counter, \"===\\n\", generated_text)\n","    counter = counter + 1\n","    print(\"PROCESSED ANSWER:\\n\", answer)\n","\n","  log = log + answer + \"\\n\"\n","  if show_log:\n","    print(\"LOG (USER SEES THIS):\\n\", log)\n"]},{"cell_type":"markdown","metadata":{"id":"VfyvtFpq0XSs"},"source":["When you're happy with your results, save your `finetuned` folder to a safe place and use it locally!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"gpt-chatbot.ipynb","provenance":[],"mount_file_id":"1_u3wb7DOW6eisGWQpCrgX2Gegj0QqyRu","authorship_tag":"ABX9TyMdvFNThM+mqkV/CnWrLVXg"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}