{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvzkreKSwW4r"
      },
      "source": [
        "# Fiction-based chatbot 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVECH3yVv4WL"
      },
      "source": [
        "check the GPU you're using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U02W5QFcTz8O",
        "outputId": "cb4268cb-c1f1-4357-c8dc-4cc420cf3df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Dec 17 15:40:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uH0AdgRjwD8_"
      },
      "source": [
        "mount your Google Drive using the \"Mount Drive\" button in the Files panel\n",
        "\n",
        "go to your project folder, which should contain\n",
        "- `run_clm.py` - python script used to train\n",
        "- `input.txt` - the text you want to train on. If you don't have one on hand, go ahead and use [Shakespeare's plays](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
        "\n",
        "replace the path below with your own version: right-click in the browser and \"Copy path\" then paste after `%cd `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oiw8d8UKa4d_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate\n",
        "%cd /content/drive/MyDrive/projects/2022 - chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCJWLyuw7dc"
      },
      "source": [
        "## Training\n",
        "\n",
        "Skip this part if you just want to generate using an existing pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6vzC_hVx5lw"
      },
      "outputs": [],
      "source": [
        "# run this to see a list of the possible training params\n",
        "!python run_clm.py --help"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xQMn0dXrITQ6"
      },
      "source": [
        "run the training. Try different values for `num_train_epochs`, `learning_rate`, `gradient_accumulation_steps`..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKblQKDOye7I"
      },
      "outputs": [],
      "source": [
        "!python run_clm.py \\\n",
        "--model_name_or_path \"gpt2\" \\\n",
        "--train_file input.txt \\\n",
        "--do_train \\\n",
        "--fp16 \\\n",
        "--overwrite_cache \\\n",
        "--output_dir finetuned \\\n",
        "--learning_rate 2e-06 \\\n",
        "--num_train_epochs 20 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--per_device_train_batch_size 2 \\\n",
        "--save_strategy epoch \\\n",
        "--save_steps 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKz4SLaQj52"
      },
      "source": [
        "`save_steps 20` and `num_train_epochs 20` means that we train for 20 epochs, and for each one we save a checkpoint into the `finetuned` dir. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NENIzMCGzWDD"
      },
      "source": [
        "## Generate\n",
        "\n",
        "set your main params here. Start with one of the standard models and do some generation. Make sure your finetuned model isn't making things worse!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9J9EBy_znVS"
      },
      "outputs": [],
      "source": [
        "# select the type of architecture: GPT-2 or NEO\n",
        "model_type = 'gpt2' # 'gpt2' or 'neo'\n",
        "# choose the exact model name\n",
        "model_name = 'gpt2' # 'gpt2' or 'EleutherAI/gpt-neo-125M' or one of your finetuned folders: 'finetuned' etc\n",
        "# add the length of the prompt tokens to match with the mesh-tf generation\n",
        "max_length = 100\n",
        "# number of generated texts\n",
        "num_generate = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ2gT0Ws05LB"
      },
      "source": [
        "now specify your (hidden) prompt, which effectively turns your language model into a chatbot. Prompt engineering is an essential part of designing such a system!\n",
        "\n",
        "see also:\n",
        "* https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/\n",
        "* https://generative.ink/posts/methods-of-prompt-programming/\n",
        "* https://chatbotslife.com/openai-gpt-3-tricks-and-tips-72cf48e233f3\n",
        "\n",
        "a single Q+A pair is enough to format GPT/NEO as a chatbot. Try experimenting with multiple Q+A pairs. Customise `head` below as you wish; just make sure it ends in `Q: `\n",
        "\n",
        "*note: the end user WILL NOT SEE this text, it's just part of the model's internal history*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Ztg89Jp_CZY_",
        "outputId": "d799ec1b-e508-4d61-ec67-7945a3e3b79c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[A chatbot from the future.]\\n\\nQ: Where are you from?\\nA: I come from a place far away, one thousand years in the future.\\nQ: Do aliens exist?\\nA: Yes, but they live very far away.\\nQ: '"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "head = \"\"\"[A chatbot from the future.]\n",
        "\n",
        "Q: Where are you from?\n",
        "A: I come from a place far away, one thousand years in the future.\n",
        "Q: Do aliens exist?\n",
        "A: Yes, but they live very far away.\n",
        "Q: \"\"\"\n",
        "\n",
        "head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7C5tY4CEcvl"
      },
      "source": [
        "do you want the chatbot to remember Q+A exchanges while it's running?\n",
        "\n",
        "*note: if `num_generate` is not 1, it will remember the last generated answer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APHJlVU1EnTi"
      },
      "outputs": [],
      "source": [
        "remember_QA = True\n",
        "show_log = True # this is what the user will see, scrolling on the screen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43rKqpVd0oRM"
      },
      "source": [
        "run the next cells to start chatting!\n",
        "\n",
        "*note: The model may attempt to continue both sides of the conversation, so our `PROCESSED ANSWER` only retains the first generate `A:` line.*\n",
        "\n",
        "1. load your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXRYmF2wzW1o"
      },
      "outputs": [],
      "source": [
        "# https://github.com/Xirider/finetune-gpt2xl\n",
        "# credit to Niels Rogge - https://github.com/huggingface/transformers/issues/10704\n",
        "\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if model_type == 'gpt2':\n",
        "  from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  tokenizer.padding_side = \"left\"\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).to(device)\n",
        "\n",
        "if model_type == 'neo':\n",
        "  from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = GPTNeoForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).half().to(\"cuda\")\n",
        "\n",
        "if model:\n",
        "  print(\"model loaded:\", model_type, \"/\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un43aZaLFpGB"
      },
      "source": [
        "2. generate your text(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzjg3zrAFTID"
      },
      "outputs": [],
      "source": [
        "log = \"\"\n",
        "while True:\n",
        "  prompt = \"\"\n",
        "  while len(prompt) == 0:\n",
        "    prompt = input(\"Q: \")\n",
        "    prompt = str(prompt)\n",
        "  log = log + \"Q: \" + prompt + \"\\n\"\n",
        "  \n",
        "  if remember_QA:\n",
        "    text = head + log\n",
        "  else:\n",
        "    text = head + \"Q: \" + prompt + \"\\n\"\n",
        "\n",
        "  if model_type == 'gpt2':\n",
        "    encoding = tokenizer(text, padding=True, return_tensors='pt').to(device)\n",
        "    max_length = max_length + len(encoding)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **encoding,\n",
        "            num_return_sequences=num_generate,\n",
        "            do_sample=True,\n",
        "            max_length=max_length,\n",
        "            top_k=50, \n",
        "            top_p=0.95,\n",
        "            use_cache=True\n",
        "          )\n",
        "    generated_texts = tokenizer.batch_decode(\n",
        "        generated_ids, skip_special_tokens=True)\n",
        "\n",
        "  if model_type == 'neo':\n",
        "      ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "      max_length = max_length + ids.shape[1]\n",
        "      gen_tokens = model.generate(\n",
        "          ids,\n",
        "          num_return_sequences=num_generate,\n",
        "          do_sample=True,\n",
        "          max_length=max_length,\n",
        "          temperature=0.7,\n",
        "          #top_k=50, \n",
        "          #top_p=0.95,\n",
        "          use_cache=True\n",
        "      )\n",
        "      generated_texts = tokenizer.batch_decode(\n",
        "          gen_tokens, skip_special_tokens=True)\n",
        "  \n",
        "  print(generated_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W358am1KF2kA"
      },
      "source": [
        "3. postprocess the `generated_texts` (to be shown to the user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "RXMp1QvNFRxF",
        "outputId": "597baef1-3b04-4018-bbcd-c45af94a422f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model loaded: neo / neo-combinedplus\n",
            "Q: what is love?\n",
            "=== 1 ===\n",
            " A: Love is a feeling that a person has developed, a longing for a dream that is to come. It is a longing for the universe and that is why they have found you. And I found you. So I am here. I am happy. So I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here. I am here\n",
            "PROCESSED ANSWER:\n",
            " A: Love is a feeling that a person has developed, a longing for a dream that is to come. It is a longing for the universe and that is why they have found you. And I found you. So I am here. I am happy. So I am here. I am here\n",
            "=== 2 ===\n",
            " A: And if you want to know, I can give you what I think you can.\n",
            "Q: How do you think I should be?\n",
            "A: I think it's really very beautiful. I think it's a magic trick. I think it's very, very easy for me to fool people, and people are very good at it. But I'm afraid of it. You know, I can't stand for it. I'm afraid of it. I'm afraid of it.\n",
            "PROCESSED ANSWER:\n",
            " A: And if you want to know, I can give you what I think you can.\n",
            "=== 3 ===\n",
            " A: Love is something that happens in our planet, and that happens to us. It is a matter of our own choice, and it is a matter of our own evolution. It is like a kind of evolutionary process. It is what we call evolution. It is a process in which our evolution is changing our world. I don't wish to speculate on it, but I do think that at the time of the explosion, we had a great number of planets, and they were larger than life\n",
            "PROCESSED ANSWER:\n",
            " A: Love is something that happens in our planet, and that happens to us. It is a matter of our own choice, and it is a matter of our own evolution. It is like a kind of evolutionary process. It is what we call evolution. It is a process in which our evolution is changing our world. I don't wish to speculate on it, but I do think that the time of the explosion, we had a great number of planets, and they were larger than life\n",
            "LOG (USER SEES THIS):\n",
            " Q: what is love?\n",
            "A: Love is something that happens in our planet, and that happens to us. It is a matter of our own choice, and it is a matter of our own evolution. It is like a kind of evolutionary process. It is what we call evolution. It is a process in which our evolution is changing our world. I don't wish to speculate on it, but I do think that the time of the explosion, we had a great number of planets, and they were larger than life\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \"\"\"\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-89d99c2e2457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Q: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "  counter = 1\n",
        "  for generated_text in generated_texts:\n",
        "    generated_text = generated_text.split(text)[1]\n",
        "  \n",
        "    import re\n",
        "    answer = generated_text\n",
        "    # split on all punctuation, take just first after `A:`\n",
        "    # https://bobbyhadz.com/blog/python-split-string-on-punctuation\n",
        "    #answer = re.split( r'()[.!?]', answer)[0]\n",
        "    #https://stackoverflow.com/questions/40736948/regex-string-repetition-of-min-length\n",
        "    answer = re.sub(r\"(.{4,}?)\\1+\", r\"\\1\", answer)\n",
        "    answer = answer.split(\"Q:\")[0]\n",
        "    answer = answer.split(\"\\n\")[0]\n",
        "    #answer = answer + '.'\n",
        "\n",
        "    print(\"===\",counter, \"===\\n\", generated_text)\n",
        "    counter = counter + 1\n",
        "    print(\"PROCESSED ANSWER:\\n\", answer)\n",
        "\n",
        "  log = log + answer + \"\\n\"\n",
        "  if show_log:\n",
        "    print(\"LOG (USER SEES THIS):\\n\", log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfyvtFpq0XSs"
      },
      "source": [
        "When you're happy with your results, save your `finetuned` folder to a safe place and use it locally!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0a4 (tags/v3.9.0a4:6e02691, Feb 25 2020, 23:23:54) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "cf8d44a43065e6e36166eb95bcc6dc77529257c53e070c3eeb62643f7e52a51b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
